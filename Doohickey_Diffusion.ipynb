{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Doohickey V0.0.1\n",
        "---\n",
        "*it's just a doohickey i use for dinking around* <br> \n",
        "by [aicrumb](https://twitter.com/aicrumb) <br>\n",
        "\n",
        "\n",
        "<br> giant credit to [johnowhitaker](https://twitter.com/johnowhitaker) bc most of the sampling code is lifted straight from his \"*Grokking Stable Diffusion*\" notebook\n",
        "\n",
        "todo: onnx unet, check if init images are working properly (probably not), thresholding functions (the imagen dynamic function doesn't work, so todo:figure out how to get it to work)"
      ],
      "metadata": {
        "id": "HZfxL5A9rA7W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288,
          "referenced_widgets": [
            "1bb4bc6980084255be1c30209fbc006a",
            "f39b1688fe6a4e019f7982f097d95180",
            "b8851e6d8ba64baeb73eca9df28a34af",
            "47a2aa50218d4f46896b56837922eed4",
            "fceffc70ff444ce9879ac700f78c09bd",
            "9d58827fec2e4d22be4f79bea0263d03",
            "12ea850cf57d4c44afc0a42ad76577bb",
            "420e0e33a48e42299e844abb277495b1",
            "0444cebecde64699a832c9908b09cc4e",
            "7f98d44194354d86b765479a0257f16f",
            "1758d7d0f65c47c68fc55f698f4e3810",
            "38f1302baccc4ea38b668ca8ed534ad1",
            "4fdca4f868a54c909e896f7d68fcf7eb",
            "2c769b3c018c42fbb2cca464f32dea50"
          ]
        },
        "id": "5dbNsZ38fPsy",
        "outputId": "22ee61b9-3159-408d-d53e-dcfcbc5bdc55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries already installed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bb4bc6980084255be1c30209fbc006a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Install required libraries / Log in to ðŸ¤—\n",
        "import os\n",
        "if not os.path.exists(\"installed.txt\"):\n",
        "    # red lines, it's fines, that's what i always say\n",
        "    !pip install transformers diffusers lpips -q\n",
        "    # !pip install git+https://github.com/openai/CLIP -q\n",
        "    !pip install open_clip_torch -q\n",
        "    !pip install wget -q\n",
        "    !cat \"test\" > installed.txt\n",
        "    print(\"Installed libraries\")\n",
        "else:\n",
        "    print(\"Libraries already installed.\")\n",
        "\n",
        "#@markdown base stable diffusion is \"CompVis/stable-diffusion-v1-4\" \n",
        "# if anyone has a script to convert the ckpt files into a format like the other huggingface models, i'd love the trinart model here\n",
        "model_name = \"CompVis/stable-diffusion-v1-4\" #@param [\"hakurei/waifu-diffusion\", \"CompVis/stable-diffusion-v1-4\", \"CompVis/stable-diffusion-v1-3\", \"CompVis/stable-diffusion-v1-2\", \"CompVis/stable-diffusion-v1-1\", \"rinna/japanese-stable-diffusion\"]\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "5jxDUlP7fyld"
      },
      "outputs": [],
      "source": [
        "#@title Import libraries\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
        "from IPython.display import Image, display\n",
        "from tqdm.auto import tqdm, trange\n",
        "from torch import autocast\n",
        "import PIL.Image as PImage\n",
        "import numpy\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as f\n",
        "import random\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Set device\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "offload_device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "aIoOVc4NgICX"
      },
      "outputs": [],
      "source": [
        "#@title Download Models / Enable Attention Slicing\n",
        "\n",
        "#@markdown attention slicing makes it so that, in pipelines, generating only requries 3.2GB of vram, at a 10% speed decrease <br>\n",
        "#@markdown reported here: https://huggingface.co/blog/diffusers-2nd-month#optimizations-for-smaller-gpus\n",
        "\n",
        "attention_slicing = True #@param {\"type\":\"boolean\"}\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\", use_auth_token=True)\n",
        "\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\", use_auth_token=True)\n",
        "unet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\", use_auth_token=True)\n",
        "\n",
        "if attention_slicing:\n",
        "    slice_size = unet.config.attention_head_dim // 2\n",
        "    unet.set_attention_slice(slice_size)\n",
        "\n",
        "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "\n",
        "vae = vae.to(offload_device).half()\n",
        "text_encoder = text_encoder.to(offload_device).half()\n",
        "unet = unet.to(torch_device).half()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ihwTrK4xg-38",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Set up generation loop\n",
        "\n",
        "to_tensor_tfm = transforms.ToTensor()\n",
        "\n",
        "# mismatch of tons of image encoding / decoding / loading functions i cant be asked to clean up right now\n",
        "\n",
        "def pil_to_latent(input_im):\n",
        "  # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
        "  with torch.no_grad():\n",
        "      with autocast(\"cuda\"):\n",
        "        latent = vae.encode(to_tensor_tfm(input_im.convert(\"RGB\")).unsqueeze(0).to(torch_device)*2-1).latent_dist # Note scaling\n",
        "#   print(latent)\n",
        "  return 0.18215 * latent.mode() # or .mean or .sample\n",
        "\n",
        "def latents_to_pil(latents):\n",
        "  # bath of latents -> list of images\n",
        "  latents = (1 / 0.18215) * latents\n",
        "  with torch.no_grad():\n",
        "    image = vae.decode(latents)\n",
        "  image = (image / 2 + 0.5).clamp(0, 1)\n",
        "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "  images = (image * 255).round().astype(\"uint8\")\n",
        "  pil_images = [Image.fromarray(image) for image in images]\n",
        "  return pil_images\n",
        "\n",
        "def get_latent_from_url(url, size=(512,512)):\n",
        "    response = requests.get(url)\n",
        "    img = PImage.open(BytesIO(response.content))\n",
        "    img = img.resize(size).convert(\"RGB\")\n",
        "    latent = pil_to_latent(img)\n",
        "    return latent\n",
        "\n",
        "def scale_and_decode(latents):\n",
        "    with autocast(\"cuda\"):\n",
        "        # scale and decode the image latents with vae\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        with torch.no_grad():\n",
        "            image = vae.decode(latents).sample.squeeze(0)\n",
        "        image = f.to_pil_image((image / 2 + 0.5).clamp(0, 1))\n",
        "        return image\n",
        "\n",
        "def fetch(url_or_path):\n",
        "        if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "            r = requests.get(url_or_path)\n",
        "            r.raise_for_status()\n",
        "            fd = io.BytesIO()\n",
        "            fd.write(r.content)\n",
        "            fd.seek(0)\n",
        "            return PImage.open(fd).convert('RGB')\n",
        "        return PImage.open(open(url_or_path, 'rb')).convert('RGB')\n",
        "\n",
        "\"\"\"\n",
        "grabs all text up to the first occurrence of ':' \n",
        "uses the grabbed text as a sub-prompt, and takes the value following ':' as weight\n",
        "if ':' has no value defined, defaults to 1.0\n",
        "repeats until no text remaining\n",
        "\"\"\"\n",
        "def split_weighted_subprompts(text, split=\":\"):\n",
        "    remaining = len(text)\n",
        "    prompts = []\n",
        "    weights = []\n",
        "    while remaining > 0:\n",
        "        if split in text:\n",
        "            idx = text.index(split) # first occurrence from start\n",
        "            # grab up to index as sub-prompt\n",
        "            prompt = text[:idx]\n",
        "            remaining -= idx\n",
        "            # remove from main text\n",
        "            text = text[idx+1:]\n",
        "            # find value for weight \n",
        "            if \" \" in text:\n",
        "                idx = text.index(\" \") # first occurence\n",
        "            else: # no space, read to end\n",
        "                idx = len(text)\n",
        "            if idx != 0:\n",
        "                try:\n",
        "                    weight = float(text[:idx])\n",
        "                except: # couldn't treat as float\n",
        "                    print(f\"Warning: '{text[:idx]}' is not a value, are you missing a space?\")\n",
        "                    weight = 1.0\n",
        "            else: # no value found\n",
        "                weight = 1.0\n",
        "            # remove from main text\n",
        "            remaining -= idx\n",
        "            text = text[idx+1:]\n",
        "            # append the sub-prompt and its weight\n",
        "            prompts.append(prompt)\n",
        "            weights.append(weight)\n",
        "        else: # no : found\n",
        "            if len(text) > 0: # there is still text though\n",
        "                # take remainder as weight 1\n",
        "                prompts.append(text)\n",
        "                weights.append(1.0)\n",
        "            remaining = 0\n",
        "    print(prompts, weights)\n",
        "    return prompts, weights \n",
        "\n",
        "\n",
        "# from some stackoverflow comment\n",
        "import numpy as np\n",
        "def lerp(a, b, x):\n",
        "    \"linear interpolation\"\n",
        "    return a + x * (b - a)\n",
        "def fade(t):\n",
        "    \"6t^5 - 15t^4 + 10t^3\"\n",
        "    return 6 * t**5 - 15 * t**4 + 10 * t**3\n",
        "def gradient(h, x, y):\n",
        "    \"grad converts h to the right gradient vector and return the dot product with (x,y)\"\n",
        "    vectors = np.array([[0, 1], [0, -1], [1, 0], [-1, 0]])\n",
        "    g = vectors[h % 4]\n",
        "    return g[:, :, 0] * x + g[:, :, 1] * y\n",
        "def perlin(x, y, seed=0):\n",
        "    # permutation table\n",
        "    np.random.seed(seed)\n",
        "    p = np.arange(256, dtype=int)\n",
        "    np.random.shuffle(p)\n",
        "    p = np.stack([p, p]).flatten()\n",
        "    # coordinates of the top-left\n",
        "    xi, yi = x.astype(int), y.astype(int)\n",
        "    # internal coordinates\n",
        "    xf, yf = x - xi, y - yi\n",
        "    # fade factors\n",
        "    u, v = fade(xf), fade(yf)\n",
        "    # noise components\n",
        "    n00 = gradient(p[p[xi] + yi], xf, yf)\n",
        "    n01 = gradient(p[p[xi] + yi + 1], xf, yf - 1)\n",
        "    n11 = gradient(p[p[xi + 1] + yi + 1], xf - 1, yf - 1)\n",
        "    n10 = gradient(p[p[xi + 1] + yi], xf - 1, yf)\n",
        "    # combine noises\n",
        "    x1 = lerp(n00, n10, u)\n",
        "    x2 = lerp(n01, n11, u)  # FIX1: I was using n10 instead of n01\n",
        "    return lerp(x1, x2, v)  # FIX2: I also had to reverse x1 and x2 here\n",
        "\n",
        "def sample(args):\n",
        "    global text_encoder # uugghhhghhghgh\n",
        "    global vae # UUGHGHHGHGH\n",
        "    global unet # .hggfkgjks;ldjf\n",
        "    # prompt = args.prompt\n",
        "    prompts, weights = split_weighted_subprompts(args.prompt)\n",
        "    h,w = args.size\n",
        "    steps = args.steps\n",
        "    scale = args.scale\n",
        "    classifier_guidance = args.classifier_guidance\n",
        "    use_init = len(args.init_img)>1\n",
        "    if args.seed!=-1:\n",
        "        seed = args.seed\n",
        "        generator = torch.manual_seed(seed)\n",
        "    else:\n",
        "        seed = random.randint(0,10_000)\n",
        "        generator = torch.manual_seed(seed)\n",
        "    print(f\"Generating with seed {seed}...\")\n",
        "    \n",
        "    # tokenize / encode text\n",
        "    tokens = [tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\") for prompt in prompts]\n",
        "    with torch.no_grad():\n",
        "        # move CLIP to cuda\n",
        "        text_encoder = text_encoder.to(torch_device)\n",
        "        text_embeddings = [text_encoder(tok.input_ids.to(torch_device))[0].unsqueeze(0) for tok in tokens]\n",
        "        text_embeddings = [text_embeddings[i]*weights[i] for i in range(len(text_embeddings))]\n",
        "        text_embeddings = torch.cat(text_embeddings, 0).sum(0)\n",
        "        max_length = 77\n",
        "        uncond_input = tokenizer(\n",
        "            [\"\"], padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "        )\n",
        "        uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
        "        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "        # move it back to CPU so there's more vram for generating\n",
        "        text_encoder = text_encoder.to(offload_device)\n",
        "    images = []\n",
        "\n",
        "    if args.lpips_guidance:\n",
        "        import lpips\n",
        "        lpips_model = lpips.LPIPS(net='vgg').to(torch_device)\n",
        "        init = to_tensor_tfm(fetch(args.init_img).resize(args.size)).to(torch_device)\n",
        "\n",
        "    for i in trange(args.batches):\n",
        "        with autocast(\"cuda\"):\n",
        "            unet = unet.to(torch_device)\n",
        "            scheduler.set_timesteps(steps)\n",
        "            if not use_init or args.start_step==0:\n",
        "                latents = torch.randn(\n",
        "                    (1, unet.in_channels, h//8, w//8),\n",
        "                    generator=generator\n",
        "                )\n",
        "                latents = latents.to(torch_device)\n",
        "                latents = latents * scheduler.sigmas[0]\n",
        "                start_step = args.start_step\n",
        "            else:\n",
        "                # Start step\n",
        "                start_step = args.start_step -1\n",
        "                start_sigma = scheduler.sigmas[start_step]\n",
        "                start_timestep = int(scheduler.timesteps[start_step])\n",
        "\n",
        "                # Prep latents\n",
        "                vae = vae.to(torch_device)\n",
        "                encoded = get_latent_from_url(args.init_img)\n",
        "                if not classifier_guidance:\n",
        "                    vae = vae.to(offload_device)\n",
        "\n",
        "                noise = torch.randn_like(encoded)\n",
        "                sigmas = scheduler.match_shape(scheduler.sigmas[start_step], noise)\n",
        "                noisy_samples = encoded + noise * sigmas\n",
        "\n",
        "                latents = noisy_samples.to(torch_device).half()\n",
        "            \n",
        "            if args.perlin_multi != 0:\n",
        "                linx = np.linspace(0, 5, h // 8, endpoint=False)\n",
        "                liny = np.linspace(0, 5, w // 8, endpoint=False)\n",
        "                x, y = np.meshgrid(liny, linx)\n",
        "                p = [np.expand_dims(perlin(x, y, seed=i), 0) for i in range(4)] # reproducable seed\n",
        "                p = np.concatenate(p, 0)\n",
        "                p = torch.tensor(p).unsqueeze(0).cuda()\n",
        "                latents = latents + (p * args.perlin_multi).to(torch_device).half()\n",
        "\n",
        "                \n",
        "            for i, t in tqdm(enumerate(scheduler.timesteps), total=steps):\n",
        "                if i > start_step:\n",
        "                    latent_model_input = torch.cat([latents]*2)\n",
        "                    sigma = scheduler.sigmas[i]\n",
        "                    latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "                    \n",
        "                    # cfg\n",
        "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "                    # cg\n",
        "                    if classifier_guidance:\n",
        "                        # vae = vae.to(torch_device)\n",
        "                        if vae.device != latents.device:\n",
        "                            vae = vae.to(latents.device)\n",
        "                        latents = latents.detach().requires_grad_()\n",
        "                        latents_x0 = latents - sigma * noise_pred\n",
        "                        denoised_images = vae.decode((1 / 0.18215) * latents_x0).sample / 2 + 0.5\n",
        "                        if args.loss_scale != 0:\n",
        "                            loss = args.loss_fn(denoised_images) * args.loss_scale\n",
        "                        else:\n",
        "                            loss = 0\n",
        "                            init_losses = lpips_model(denoised_images, init)\n",
        "                            loss = loss + init_losses.sum() * args.lpips_scale\n",
        "\n",
        "                        cond_grad = -torch.autograd.grad(loss, latents)[0]\n",
        "                        latents = latents.detach() + cond_grad * sigma**2\n",
        "                        # vae = vae.to(offload_device)\n",
        "\n",
        "                    latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
        "        vae = vae.to(torch_device)\n",
        "        output_image = scale_and_decode(latents)\n",
        "        vae = vae.to(offload_device)\n",
        "        images.append(output_image)\n",
        "\n",
        "        import gc\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        images[-1].save(\"temp.png\")\n",
        "        display(Image(\"temp.png\"))\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load textual-inversion concepts from ðŸ¤— hub\n",
        "load_full_concepts_library = False #@param {\"type\":\"boolean\"}\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "import wget\n",
        "api = HfApi()\n",
        "def load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer, token=None):\n",
        "        loaded_learned_embeds = torch.load(learned_embeds_path, map_location=\"cpu\")\n",
        "        \n",
        "        # separate token and the embeds\n",
        "        trained_token = list(loaded_learned_embeds.keys())[0]\n",
        "        embeds = loaded_learned_embeds[trained_token]\n",
        "\n",
        "        # cast to dtype of text_encoder\n",
        "        dtype = text_encoder.get_input_embeddings().weight.dtype\n",
        "        embeds.to(dtype)\n",
        "\n",
        "        # add the token in tokenizer\n",
        "        token = token if token is not None else trained_token\n",
        "        num_added_tokens = tokenizer.add_tokens(token)\n",
        "        i = 1\n",
        "        # while(num_added_tokens == 0):\n",
        "        #     print(f\"The tokenizer already contains the token {token}.\")\n",
        "        #     token = f\"{token[:-1]}-{i}>\"\n",
        "        #     print(f\"Attempting to add the token {token}.\")\n",
        "        #     num_added_tokens = tokenizer.add_tokens(token)\n",
        "        #     i+=1\n",
        "        \n",
        "        # resize the token embeddings\n",
        "        text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "        \n",
        "        # get the id for the token and assign the embeds\n",
        "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "        text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n",
        "        return token\n",
        "\n",
        "\n",
        "if load_full_concepts_library:\n",
        "    models_list = api.list_models(author=\"sd-concepts-library\", sort=\"likes\", direction=-1)\n",
        "    models = []\n",
        "\n",
        "    print(\"Setting up the public library\")\n",
        "    for model in models_list:\n",
        "        model_content = {}\n",
        "        model_id = model.modelId\n",
        "        model_content[\"id\"] = model_id\n",
        "        embeds_url = f\"https://huggingface.co/{model_id}/resolve/main/learned_embeds.bin\"\n",
        "        os.makedirs(model_id,exist_ok = True)\n",
        "        if not os.path.exists(f\"{model_id}/learned_embeds.bin\"):\n",
        "            try:\n",
        "                wget.download(embeds_url, out=model_id)\n",
        "            except:\n",
        "                continue\n",
        "        token_identifier = f\"https://huggingface.co/{model_id}/raw/main/token_identifier.txt\"\n",
        "        response = requests.get(token_identifier)\n",
        "        token_name = response.text\n",
        "        print(f\"added {token_name}\")\n",
        "        concept_type = f\"https://huggingface.co/{model_id}/raw/main/type_of_concept.txt\"\n",
        "        response = requests.get(concept_type)\n",
        "        concept_name = response.text\n",
        "        model_content[\"concept_type\"] = concept_name\n",
        "        images = []\n",
        "        model_content[\"images\"] = images\n",
        "\n",
        "        learned_token = load_learned_embed_in_clip(f\"{model_id}/learned_embeds.bin\", text_encoder, tokenizer, token_name)\n",
        "        model_content[\"token\"] = learned_token\n",
        "        models.append(model_content)\n",
        "\n",
        "specific_concepts = [] #@param\n",
        "\n",
        "models = []\n",
        "for model in specific_concepts:\n",
        "    model_content = {}\n",
        "    model_content[\"id\"] = model\n",
        "    embeds_url = f\"https://huggingface.co/{model}/resolve/main/learned_embeds.bin\"\n",
        "    os.makedirs(model,exist_ok = True)\n",
        "    if not os.path.exists(f\"{model}/learned_embeds.bin\"):\n",
        "        try:\n",
        "            wget.download(embeds_url, out=model)\n",
        "        except:\n",
        "            continue\n",
        "    token_identifier = f\"https://huggingface.co/{model}/raw/main/token_identifier.txt\"\n",
        "    response = requests.get(token_identifier)\n",
        "    token_name = response.text\n",
        "    print(f\"added {token_name}\")\n",
        "\n",
        "    concept_type = f\"https://huggingface.co/{model}/raw/main/type_of_concept.txt\"\n",
        "    response = requests.get(concept_type)\n",
        "    concept_name = response.text\n",
        "    model_content[\"concept_type\"] = concept_name\n",
        "    images = []\n",
        "    model_content[\"images\"] = images\n",
        "\n",
        "    learned_token = load_learned_embed_in_clip(f\"{model}/learned_embeds.bin\", text_encoder, tokenizer, token_name)\n",
        "    model_content[\"token\"] = learned_token\n",
        "    models.append(model_content)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "v4_YYtPhO8Do"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fun tips\n",
        "1. if you want to generate from scratch but fit to a general pose / orientation (this works well about 1/8 of the time):\n",
        "set an init image with the pose, set start step to 0, turn on lpips guidance and classifier guidance (needed for lpips to work), turn loss_scale to 0 and experiment with lpips_scale.\n",
        "2. dont use clip guidance 'oh the headaches you'll have' coming soon to a dr seuss retailer near you\n",
        "3. the 'waifu diffusion' model works best on danbooru tags (that's what it was trained on)"
      ],
      "metadata": {
        "id": "Zezy_eO1zdDd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PytCwKXCmPid",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Generate**\n",
        "#@markdown ---\n",
        "#@markdown ### General Parameters\n",
        "#@markdown `prompt` can be split into weighted prompts with `prompt1:weight prompt2:weight` <br>\n",
        "#@markdown `init_img` should be a link to an image, but can be left blank <br>\n",
        "#@markdown `size` should be a list with height first then width (stable diffusion is trained on images with size `[512,512]`)<br>\n",
        "#@markdown `steps` can be anything, around 40-80 works for most cases <br>\n",
        "#@markdown `start_step` is how many steps you skip into the generation, depends on your `steps`, normally use with an `init_img` <br>\n",
        "#@markdown `perlin_multi` is how much perlin noise to add to the inital image, 0.45 is the maximum i'd use ever and 0.2 seems like a reasonable in-between value <br>\n",
        "#@markdown `scale` normally should be around 7.5 <br>\n",
        "#@markdown `seed` can be -1 for a random seed (which will be printed) or set to whatever integer you want <br>\n",
        "#@markdown `batches` is how many images you want to generate in total <br>\n",
        "\n",
        "\n",
        "# idk how people normally do this and i cba to look\n",
        "prompt = \"\" #@param {\"type\":\"string\"}\n",
        "init_img = \"\" #@param {\"type\":\"string\"}\n",
        "size = [512, 512] #@param\n",
        "steps = 50 #@param\n",
        "start_step = 0 #@param\n",
        "perlin_multi = 0. #@param\n",
        "scale = 1 #@param\n",
        "seed = 8156 #@param\n",
        "batches = 8 #@param\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ### Classifier Guidance\n",
        "#@markdown `classifier_guidance` is whether or not to use the loss function in the previous cell to guide the image (slows down image generation a lot) <br>\n",
        "#@markdown `lpips_guidance` is for if you're using an init_img, it'll let you start closer to the beginning while trying to keep the overall shapes similar\n",
        "#@markdown `lpips_scale` is similar to `loss_scale` but it's how much to push the model to keep the shapes the same <br>\n",
        "#@markdown `loss_scale` is how much to guide according to that loss function <br>\n",
        "#@markdown `clip_text_prompt` is a prompt for CLIP to optimize towards, if using classifier guidance (supports weighting with `prompt:weight`) <br>\n",
        "#@markdown `clip_image_prompt` is an image url for CLIP to optimize towards if using classifier guidance (supports weighting with `url|weight` because of colons coming up in urls) <br>\n",
        "#@markdown for `clip_model_name` and `clip_model_pretrained` check out the openclip repository https://github.com/mlfoundations/open_clip <br>\n",
        "#@markdown `cutn` is the amount of permutations of the image to show to clip (can help with stability) <br>\n",
        "#@markdown *you cannot use the textual inversion tokens with the clip text prompt*  <br>\n",
        "#@markdown *also clip guidance sucks for most things except removing very small details that dont make sense*\n",
        "classifier_guidance = False #@param {\"type\":\"boolean\"}\n",
        "lpips_guidance = False #@param {\"type\":\"boolean\"}\n",
        "lpips_scale = 0 #@param\n",
        "loss_scale = 8 #@param\n",
        "\n",
        "class BlankClass():\n",
        "    def __init__(self):\n",
        "        bruh = 'BRUH'\n",
        "args = BlankClass()\n",
        "args.prompt = prompt\n",
        "args.init_img = init_img\n",
        "args.size = size \n",
        "args.steps = steps \n",
        "args.start_step = start_step \n",
        "args.scale = scale\n",
        "args.perlin_multi = perlin_multi\n",
        "args.seed = seed\n",
        "args.batches = batches \n",
        "args.classifier_guidance = classifier_guidance\n",
        "args.lpips_guidance = lpips_guidance\n",
        "args.lpips_scale = lpips_scale\n",
        "args.loss_scale = loss_scale\n",
        "\n",
        "if args.classifier_guidance:\n",
        "    # import clip\n",
        "    import open_clip as clip\n",
        "    from torch import nn\n",
        "    import torch.nn.functional as F\n",
        "    import io\n",
        "\n",
        "    class MakeCutouts(nn.Module):\n",
        "        def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "            super().__init__()\n",
        "            self.cut_size = cut_size\n",
        "            self.cutn = cutn\n",
        "            self.cut_pow = cut_pow\n",
        "\n",
        "        def forward(self, input):\n",
        "            sideY, sideX = input.shape[2:4]\n",
        "            max_size = min(sideX, sideY)\n",
        "            min_size = min(sideX, sideY, self.cut_size)\n",
        "            cutouts = []\n",
        "            for _ in range(self.cutn):\n",
        "                size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "                offsety = torch.randint(0, sideY - size + 1, ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "                cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "            return torch.cat(cutouts)\n",
        "    # make_cutouts = MakeCutouts(224, 16)\n",
        "\n",
        "    clip_text_prompt = \"\" #@param {\"type\":\"string\"}\n",
        "    clip_image_prompt = \"\" #@param {\"type\":\"string\"}\n",
        "\n",
        "    if loss_scale != 0:\n",
        "        # clip_model = clip.load(\"ViT-B/32\", jit=False)[0].eval().requires_grad_(False).to(torch_device)\n",
        "        clip_model_name = \"ViT-H-14\" #@param {\"type\":\"string\"}\n",
        "        clip_model_pretrained = \"laion2b_s32b_b79k\" #@param {\"type\":\"string\"}\n",
        "        clip_model, _, preprocess = clip.create_model_and_transforms(clip_model_name, pretrained=clip_model_pretrained)\n",
        "        clip_model = clip_model.eval().requires_grad_(False).to(torch_device)\n",
        "\n",
        "        cutn = 3 #@param\n",
        "        make_cutouts = MakeCutouts(clip_model.visual.image_size if type(clip_model.visual.image_size)!= tuple else clip_model.visual.image_size[0], cutn)\n",
        "\n",
        "    target = None\n",
        "    if len(clip_text_prompt) > 1:\n",
        "        clip_text_prompt, clip_text_weights = split_weighted_subprompts(clip_text_prompt)\n",
        "        target = clip_model.encode_text(clip.tokenize(clip_text_prompt).to(torch_device)) * torch.tensor(clip_text_weights).view(len(clip_text_prompt), 1).to(torch_device)\n",
        "    if len(clip_image_prompt) > 1:\n",
        "        clip_image_prompt, clip_image_weights = split_weighted_subprompts(clip_image_prompt, split=\"|\")\n",
        "        # pesky spaces\n",
        "        clip_image_prompt = [p.replace(\" \", \"\") for p in clip_image_prompt]\n",
        "        images = [fetch(image) for image in clip_image_prompt]\n",
        "        images = [f.to_tensor(i).unsqueeze(0) for i in images]\n",
        "        images = [make_cutouts(i) for i in images]\n",
        "        encodings = [clip_model.encode_image(i.to(torch_device)).mean(0) for i in images]\n",
        "        \n",
        "        for i in range(len(encodings)):\n",
        "            encodings[i] = (encodings[i] * clip_image_weights[i]).unsqueeze(0)\n",
        "        # print(encodings.shape)\n",
        "        encodings = torch.cat(encodings, 0)\n",
        "        encoding = encodings.sum(0)\n",
        "\n",
        "        if target!=None:\n",
        "            target = target + encoding\n",
        "        else:\n",
        "            target = encoding\n",
        "        target = target.half().to(torch_device)\n",
        "\n",
        "    # free a little memory, we dont use the text encoder after this so just delete it\n",
        "    clip_model.transformer = None\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    def spherical_distance(x, y):\n",
        "        x = F.normalize(x, dim=-1)\n",
        "        y = F.normalize(y, dim=-1)\n",
        "        l = (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2).mean()\n",
        "        return l \n",
        "\n",
        "    def loss_fn(x):\n",
        "        with torch.autocast(\"cuda\"):\n",
        "            cutouts = make_cutouts(x)\n",
        "            encoding = clip_model.encode_image(cutouts.float()).half()\n",
        "            loss = spherical_distance(encoding, target)\n",
        "            return loss.mean()\n",
        "\n",
        "    args.loss_fn = loss_fn\n",
        "\n",
        "output = sample(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qn0ZI91R2gAw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1bb4bc6980084255be1c30209fbc006a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f39b1688fe6a4e019f7982f097d95180",
              "IPY_MODEL_b8851e6d8ba64baeb73eca9df28a34af",
              "IPY_MODEL_47a2aa50218d4f46896b56837922eed4",
              "IPY_MODEL_fceffc70ff444ce9879ac700f78c09bd"
            ],
            "layout": "IPY_MODEL_9d58827fec2e4d22be4f79bea0263d03"
          }
        },
        "f39b1688fe6a4e019f7982f097d95180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12ea850cf57d4c44afc0a42ad76577bb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_420e0e33a48e42299e844abb277495b1",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b8851e6d8ba64baeb73eca9df28a34af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_0444cebecde64699a832c9908b09cc4e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7f98d44194354d86b765479a0257f16f",
            "value": ""
          }
        },
        "47a2aa50218d4f46896b56837922eed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_1758d7d0f65c47c68fc55f698f4e3810",
            "style": "IPY_MODEL_38f1302baccc4ea38b668ca8ed534ad1",
            "tooltip": ""
          }
        },
        "fceffc70ff444ce9879ac700f78c09bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fdca4f868a54c909e896f7d68fcf7eb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2c769b3c018c42fbb2cca464f32dea50",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "9d58827fec2e4d22be4f79bea0263d03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "12ea850cf57d4c44afc0a42ad76577bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "420e0e33a48e42299e844abb277495b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0444cebecde64699a832c9908b09cc4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f98d44194354d86b765479a0257f16f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1758d7d0f65c47c68fc55f698f4e3810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38f1302baccc4ea38b668ca8ed534ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4fdca4f868a54c909e896f7d68fcf7eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c769b3c018c42fbb2cca464f32dea50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}